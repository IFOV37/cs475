CS 475 Notes
Spring 2018


Project Notes
================================================================================


double time0 = omp_get_wtime();
	-> OpenMP call
	-> used to get time



USE SCRIPTING TO MAKE THIS SHIT EASIER TO TEST DIFFERENT NUMBERS!


Simple OpenMP intro
=============================================================================

Open Multi-Processing

	-> mostly compiler directives "pragmas"

#pragma omp directive [clause]

turn it on in Linux:
	g++ -o proj proj.cpp -lm -fopenmp

For windows turn on, see slide 4 of the lecture
	-> in config props in VS


Coolest thing (possibly):


	omp_set_num_threads(NUMT);  // sets how many threads will be in the thread pool

	...

	#pragma omp parallel for
	for(int i = 0; i < arraySize; i++)
	{....}

	-> code starts as a single thread,
	then this uses NUMT threads to execute the loop simultaneously



Parallel programming background infomation

Lecture 1 (A?)
=======================================================

3 reasons:
	1) increase performance (more work same time)
	2) increase performance (take lest time to do the same amt of work)
	3) make some programming tasks more convenient to implement


3 types of parallelism:

	1) instruction level parallelism (ILP)
		-> out of order execution
			-> logic looks ahead to grab some code that it can execute while waiting on something that might be blocked (such as fetchign something that isn't in cache (getting from memory))

			-> if compiler does this = Static ILP
			-> if CPU chip = Dynamic ILP

		Note: we can't control this


	2) Data Level Parallelism (DLP)
		-> executing the same instructions on different parts of the data
			If we have a for loop on a large array and same instructions.. we can break this up!
				-> multiple threads on multiple cores

			We will use OpenMP with this!


	3) Thread Level Parallelism (TLP)
		-> executing DIFFERENT instructions
			Ex: variety of incoming transaction requests
		-> implies you have more threads than cores
		-> thread exec switches when a thread blocks or uses up its time slice

State:
	-> Registers
	-> Program Counter
	-> Stack Pointer

	(We will visit this again in multi-core programming)


Process:
	executes a program in memory

	keeps a state (see above)


Thread:
	separate indp procs, all execing a common pgm and sharing memory
		-> each thread has its own state

	-> only thing different is each thread has it's own stack

	can time share on a single processors
		(don't need multiple processors, but you can - MULTICORE COMING SOON!)



lecture 2
=============================================================================

Conflicts in multithreaded programs.. Thread safety
	-> can take a function that be used many times and won't alter the state of something

	What is not thread safe?

		Ex: char *strtok (char * str, const char * delims);

		-> this keeps internal state info.. if we alter that state in a 2nd thread, this screws with the first threads info.

		strtok_r returns the internal state (new version of strtok)
			-> so you can pass back when ready.. 'r' stands for "re-entrant"



Deadlock Fault Problems:
	Deadlock: 2 threads are each waiting for the other to do something


Race Condition Fault:
	-> where it matters which trhead gets to a particular piece of code first
	-> often comes about when on thread is modifying a var while the other thread is in the midst of using it

	Often fixed with Mutual Exclusion Locks (Mutexes)
		-> lock the stuff while we do something, then unlock and let the other grab it and do its thing

volatile keyword: used to let the compiler know that another thread might be changing a variable "in the background", so don't make any assumptions about what can be optimized away.
	Ex:  volatile int val = 0;
		w/o volatile a good compiler would know that val == 0



restrict keyword: w/o this could create race condition in the "optimized", but wrong code.
	-> this is a "promise" that the vars will never point to the same memory location


lecture 3
=======================================================================================

Functional (or Task) Decomposition:
	-> breaking a task into sub-tasks that represent separate fns
	-> primarily indpt


Domain (or Data) Decomposition
	-> breaking task into sub-tasks that represent separate sections of data
	Ex: big matrix solutions


Atomic: An operation that takes place to completion w/ no chance of being interrupted by another thread

Deterministic: same set of inputs always gives the same outputs

Reduction: Combining the results from multiple threads into a single sum or product, continuing to use multithreading. Typically performed so that it takes O(log_2 N) time instead of O(N) time

Fine-grained parallelism: breaking a task into lots of small tasks

Coarse-grained parallelism: breaking a task up into small number of large tasks

Barrier: A point in the pgm where ALL threads must reach before any of them are allowed to proceed


Fork-join: operationg whr multiple threads are created from a main thread.  All of those forked threads are expected to eventually finish and thus "join back up" w/ the main thread

Shared Variable: After a fork operation, a var which is shared among threads, i.e., has a single value

Private variable: After a fork op, a var which has a private copy w/in each thread

Static scheduling: dividng the total # of tasks T up so that each of N available threads has T/N sub-tasks to do

Dynamic scheduling: dividing the total # of tasks T up so that each of N available threads has less than T/N sub-tasks to do, and then doling out the remaining tasks to threads as they become available

Speed-up(N)   T_1 / T_N

Speed-up Efficiency Speed-up(N) / N






WEEK 2
---------------------------------------------------------------------------------
(lecture 4 was week 2 welcome)

lecture 5 - Open MP A
=================================================================================

both directive and library-based

shared executable, global memory, and heap

Not totally deterministic, so no guarantee identical behavior accross venders or hardware
	-> even multiple runs on same hardware

Doesn't guarantee order of execution either



Open MP usage:
g++ -o proj proj.cpp -lm -fopenmp

for intel optimization:
icpc -o proj proj.cpp -lm -fopenmp -align -qopt-report=3 -qopt-report-phase=vec


set num threads:
omp_set_num_threads(num);

ask how many cores this pgm has access to:
num = omp_get_num_procs();

how many threads using right now?:
num = omp_get_num_threads();

which thread one is?
me = omp_get_thread_num();



#pragma omp parallel default(none)
	-> creates the team of threads to exec
	-> threads won't be in order


lecture 6 - Open MP B
=========================================================================================


#pragma omp parallel for default(none)
	-> creates team of threads and divides them among the for loop passes w/ 1 line of code
	-> default(none) - could remove the implied barrier with this optional item

	default(none) aka no default
		-> ensures variables are explicitly declared inside and outside the threaded portion w/ shared or private

	private(x) - means each thread has its own copy of var x

	shared(x) - all threads shared a common x


	Ex:
	#pragma omp parallel for default(none), private(i,j), shared(x)


if you don't schedule with OpenMP, it defaults to static
	schedule(static [,chunksize])
	schedule(dynamic [,chunksize])
			-> chunksize defaults to 1


lecture 6 - Open MP C
=========================================================================================


Where can we see problems?

	Ex: adding a sum in the for loop that has be made to operate in parallel

		-> no guarantee the sum line is exec'd correctly

		-> no guarantee each thread will finish this line b4 some other thread interrupts it
			multiple Assembly code lines are usually is generated

		-> non-deterministic


	3 solutions:

		1)   #pragma omp atomic
			-> fixes non-deterministic problem
			-> tries to use a built in hardware instruction

		2)  #pragma omp critical
			-> disables scheduler interrupts during the critical section


				BEST METHOD
		3) #pragma omp parallel for reduction(+:sum),private(myPartialSum)
			....
			sum = sum + myPartialSum;

			-> in O(log_2 N) instead of O(N)
			-> reduction creates a sum private var for each thread
				-> adds each some in a binary fashion (hence O(log_2 N))



lecture 7 - Open MP D
===========================================================================================

Synchronization:

	Mutual Exclusion Locks (Mutexes)

		omp_init_lock(omp_lock_t *);
		omp_set_lock(omp_lock_t *);  // blocks if lock not available, sets and rtns when avail
		omp_unset_lock(omp_lock_t *);
		omp_test_lock(omp_lock_t *);  // if lock not avail, rtn 0, if avail, sets and retrns !0

Critical sections
	#pragma omp critical
		-> restricts exec to 1 thread at a time

	#pragma omp single
		-> restricts exe to a single thread ever

Barriers

	#pragma omp barrier
		-> forces each thread to wait here until all threads arrive
			-> implied w/ for loops and openMP sections, unless NOWAIT clause is used.


Creating sections of MP Code
	-> indp blocks of code, able to be assigned to separate threads if they are available

	#pragma omp parallel sections
	{
		#pragma omp section
		{
			Task 1
		}

		#pragma omp section
		{
			Task 2
		}
	}  // implied barrier here



OpenMP Tasks - works like a section, but created on the fly

	#pragma omp task
		code here

	Force a barrier
		#pragma omp taskwait


lecture 9 - Speedups and Amdahl's law
===========================================================================

Using n processors, speedup_n is:

	speedup_n = T1 / Tn... actually, I think this should be flipped with Tn on top. as speedup should be > 1
						-> if you take the inverse of this number, you get something > 1
							-> which is just the number of Tn / T1 anyway.

Speedup Efficiency_n:
		E_n = Speedup_n / n  (could be as high as 1)




Amdahl's Law:
	-> mainly used to predict the theoretical max speedup of a program processing using multiple processors

	Speedup_n = T1 / Tn
		= 1 /  [ (F_parallel / n) + F_sequential ]

		= 1 / [ (F_p / n) + (1 - F_p)]


			-> parallel potion can be cut down, but not sequential

	-> if we know the max F_p, we can calc the max speedup


	F_p =   n / (n-1)   * T1 - Tn / T1
		=  n / (n-1)  * ( 1 -  1 / speedup)



	maxspeedup = 1 / 1 - F_p



Gustafson's Obersvation:
	As amount of data increases, you can increase the parallel fraction
		-> data generally comes in on the parallel side rather than the sequential side
			-> it does still have to be read in, so there is some sequential, so this is somewhat theoretical

lecture 10 - Moore's Law and Multicore
===========================================================================

Moore's Law
	-> obersvation that "transistor density doubles every 1.5 years"
		-> does not necessarily mean clock speed does the same.. was a artifact of the above

After about 2003 - transistor density is still following the law, but clock speed and power cosnumption leveled off

Power Consumption (by the chip) is proportional to ClockSpeed ^ 2
	-> if you keep doubling clock speed, this would get incredibly hot
		For instance: by about mid 00's, it would be as hot as a nuclear reactor
			-> 2015 ish = sun's surface



Since we aren't increasing clock speed due to power consumption (hitting a wall), what do we do with the extra space?
	-> pack multiple processors onto the same chip
		-> this is MultiCore

MultiCore and Multithreading

MC w/o MT is a good thing.  Use to allow multiple pgms on a desktop system to always be executing concurrently.  (MC is basically more hardware aka processors)

MT w/o MC, still a good thing.  Threads can make it easier to logically have many things going on in your program at a time, and can absorb the dead-time of other threads.
	-> Largely only software

Big performance gain: use both to speed up a single program.
	-> we need to be prepared to convert our programs to run on MultiThreaded Shared Memory Multicore architectures



HT = Hyper Threading
	-> processor could have HT, but not multiple execution units
	-> not really 2 simul threads, but these 2 can swap quickly
	-> most important: count the execution units



week 3
------------------------------------------------------------------------------------

bubble sort
================================================================================

If we remove the inner for loop to see what is happening with bubble sort and parallelizing it, we basically have a bunch of conditionals inside 1 loop
	-> 1 conditional could be on one thread and the next on a different.
		-> since we have no control over thread scheduling, this is a race condition

	We would then do the pragma omp statement on the inner loop:
		#pragma omp parallel for default(none),shared(A, first)

		take note that the inner loop jumps by 2 (this looks at pairs)
			-> outer loop starts with odd or even based on first

for( int i = 0; i < N; i++ )
{
	int first = i % 2; 		// 0 if i is 0, 2, 4, ...
							// 1 if i is 1, 3, 5, ...

	#pragma omp parallel for default(none),shared(A,first)

	for( int j = first; j < N-1; j += 2 )
	{
		if( A[ j ] > A[ j+1 ] )
		{
			std::swap( A[ j ], A[ j+1 ] );
		}
	}
}


tree traversal using openmp tasks
===============================================================================
Tasks are good for DS that you don't know how big they are (LL, Trees, etc..)

Assume we want to traverse as quickly as possible
	-> BUT DO NOT CARE ABOUT ORDER


	#pragma omp parallel
	{
		#pragma omp single

		Traverse(root); // w/o this, each thread does a full traversal
						// at best = wasteful
						// at worst = dangerous
	}

	#pragma omp taskwait  // if you want to wait for all nodes to be traversed b4 proceeding


void
Traverse( Node *n )
{
	if( n->left != NULL )
	{						// with tasks, these types of things are added to a task's to do list
		#pragma omp task private(n) untied   // untied - means another open thread can steal 											this one (to work on it)
		Traverse( n->left );
	}

	if( n->right != NULL )
	{
		#pragma omp task private(n) untied
		Traverse( n->right );
	}

	#pragma omp taskwait	// if you want both branches to be taken b4 processing the parent
	// can get rid of this if we don't care as it will slow stuff down

	Process( n );
}

Summary:

Tasks get spread among the current "thread team"

Tasks can execute immediately or be deferred.  They are exec'd at "some time."

Tasks can be moved b/w threads, that is, if one thread has a backlog of tasks to do, an idel thread can come steal some workload.

Tasks are more dynamic than sections.  The task paradigm would still work if there was a variable number of children at each node.



cache A
============================================================================

Problem: the path b/w a cpu chip and off-chip memory is slow
	-> put in "intermediate memory"
		-> Hierarchical Memory systems or "Cache"

	-> cache and memory are named by "distance level" from the ALU
		L1 - smaller, faster, closer
		L2 - slightly larger, a little slower, a little further away

		-> cache brings more things in from memory so it doesn't have to go as far to get it


Example of characteristics:
	L1 cache: 32KB + 32KB  .. or Instruction and Data cache



Cache hit: when cpu asks for a value from memory and that value is already in the cache, it can get it quickly

Cache miss: cpu asks for val in mem, val not in cache, it will have to go off the chip to get it

Cache Line: while cache might be multiple kilo or megabytes, the bytes are transferred in much smaller quantities (each is called this)
	-> size of a cache line is typically just 64 bytes



Successful cache use depends on both:

Spatial Coherence:
	“If you need one memory address’s contents now, then you will probably
	also need the contents of some of the memory locations around it soon.”

Temporal Coherence:
	“If you need one memory address’s contents now, then you will probably
	also need its contents again soon.”


EX:
	If have 2 for loops (1 nested) and we are accessing rows and columns, think of a 4 x 4 grid of values.
		-> If you access the first 16 positions, this will be fast due to the values being stored in cache (across the rows)
		-> if you go down via columns, this is slower since not all the values are in cache
			-> we are accessing things out of order



cache B
==============================================================================

Which is better: array of structs or structure of arrays?

	-> if we are going to use them in groups (x, y, z), then the array of structs works well
		-> better spatial coherence

	-> if we want to take the average of numbers where we add up all the arrays
		-> better to use structure of arrays (list)


PREVIEW FOR SIMD:
	Dx[0:N] = X[0:N] - Xnow;  // a for loop from 0 to N-1



Good OOP style can be incosistent with Good Cache use
	-> new/malloc could be all over the place.. bad for spatial coherence

		-> to solve, you use new/malloc selectively
			-> don't malloc 1 at a time, do a bunch so we get a bunch of memory addresses next to each other
				-> when you add a new list element, you grab it from the ones you created

				*great spatial coherence*


Why can we get decreased performance as data sets get larger?
	-> violating temporal coherence
		-> only using every value once, so it is in cache but we never use it again
		-> arthimatic is outrunning your ability to reload

Preview of Pre-Fetching (later in SIMD)
	-> helps with temporal problem

	-> when you are in a for loop and load the 0th ele, pre-fetch the 16th

Example of where cache coherence matters:
	Matrix multiplication
		-> goes across Row of A, but down col of B

	*You don't have to do this in i, j, k order*
		-> if you did something like i, k, j
			-> goes row i of A and row k of B
				-> produces ele (i, j) of C

				*i k j has the best spatial coherence*


cache C
===============================================================================

cache architectures

N-way Set Associate - cache line from a particular block of memory can appear in a limited number of places in a cache.  Each "limited place" is called a set of cache lines  A set contains N cache lines.

	N is typically 4 for L1 and 8 or 16 for L2


64 bytes in a cache line (refresher)


Each core has its own separate L2 cache
	-> say 2 cores are operating on vars in your pgm that happen to live on the same cache line
		-> fine if they read
		-> if one writes, the other is no longer correct.

		So cache maintains 4 states
			1. Modified
			2. Exclusive
			3. Shared
			4. Invalid

			(MESI)


			Say you do this:
				1) read something with core A..
						A = exclusive

				2) core B reads val from same mem area
						A = shared
						B = shared

				3) B writes into that val, retagged as mod'd, core A re-tagged
						A = Invalid
						B = Modified

				4) A tries to read val from same part of mem, but this is invalid
					-> core B cache line has to be flushed back to mem
					-> then A cache line is re-loaded from mem
					-> now both are shared

					This is False Sharing - huge performance hit (not incorrect results)


cache D
==================================================================================


False Sharing Example Problem:

struct s
{
	float value;
	int pad[NUMPAD];  // we could set numpad=3, which would space it out enough to not 								invalidate the cache line
					// can be variable from 0 to 15 (0 basically gotten rid of by compiler)
					// setting to 7, we have 2 on each of the 1st 2 cache lines
					// setting to 15, we have 1 on each of 4 cache lines, thus best performance
} Array[4];

omp_set_num_threads( 4 );

#pragma omp parallel for
	for( int i = 0; i < 4; i++ )
	{
		for( int j = 0; j < SomeBigNumber; j++ )
		{
			Array[ i ].value = Array[ i ].value + (float)rand( );
			// this reads and writes to the same, thus invalidating the cache line
		}
	}

	* 1 kind of fix is the above... this is to pad your data structures to put key values on different cache lines.. this optimizes the efficience.. *
		-> by wasting memory, you can improve performance.. who'da thunk it
			-> "being cache aware"


Potential fix #2:

If we use local vars, instead of continguous array locations, that will spread out our writes out in memory, and to different cache lines

	-> temp local variables live on each thread's stack
		-> thus not next to each other in memory





#include <stdlib.h>
struct s
{
	float value;
} Array[4];

omp_set_num_threads( 4 );

const int SomeBigNumber = 100000000;

#pragma omp parallel for
	for( int i = 0; i < 4; i++ )
	{
		float tmp = Array[ i ].value;  // THIS GUY.. makes this a private variable that lives in 									each thread's individual stack
							// this works b/c a localized temp variable is created in each core's stack area, so little or no cache line conflict exists

		for( int j = 0; j < SomeBigNumber; j++ )
		{
			tmp = tmp + (float)rand( );
		}
		Array[ i ].value = tmp;
	}

		*this does one read and one store per exe *


malloc'ing on a cache line
	-> know each cache line starts on a fixed 64-byte doundaries lets you do this:
		top N-6 bits tell you what cache line number this address is a part of.
		btm 6 bits tell you what offset that address has w/in that cache line.

			Ex: 32-bit mem system:

			|	32-6 = 26 bits     |    6 bits |
				cache line number 		offset in that cache line

				*so, if you see a mem add whose btm 6 bits are 000000, then you know that that mem location begins a cache line*


Normal allocation of a struct where you want to mallloc an ARRAYSIZE array of them:

struct xyzw *p = (struct xyzw *) malloc( (ARRAYSIZE)*sizeof(struct xyzw) );
struct xyzw *Array = &p[0];
. . .
Array[ i ].x = 10. ;



Want to make sure that array of structs started on a cache line boundary:

unsigned char *p = (unsigned char *) malloc( (ARRAYSIZE+1)*sizeof(struct xyzw) );
int offset = (int)p & 0x3f; 						// 0x3f = bottom 6 bits are all 1’s
struct xyzw *Array = (struct xyzw *) &p[64-offset];
. . .
Array[ i ].x = 10. ;


To free, use free(p);
Not free(Array);




Week 4
======================================================================================

rabbit lecture
-----------------------------------------------------------------------------------

rabbit.engr.oregonstate.edu
	-> used explicitely for this class that we can ssh into
	-> xeon system, 2 processor, 16 cores total, 64 gb of memory

	-> attached to it is 15SMs and 2800 CUDA cores 6GB graphics card
	-> attached xeon phi 57 core system
		-> 4 way hyper threaded (224 hyperthreads)


	lscpu -> gives info about cpu


	Login:   rabbit.engr.oregonstate.edu -l yourengrusername
									^ dash el

		-> get your stuff working first, then go here

		Need to put stuff in rabbit act's .cshrc 
			-> source .cshrc

			Check out slide 7 for this week on what you need to enter.

	NOT REQUIRED TO RUN ANY TESTS ON HERE, BUT GOOD FOR INTERESTING DATA AND FUN


Prefetching lecture
--------------------------------------------------------------------------------

Prefetching -> the process go going out and reaching ahead and saying " I don't need it yet, but sometime pretty soon I'm going to need this cache line out here ahead"
	-> compiler specific

	-> used to place cache line in memory before it is to be used, thus hiding the latency of fetching from off-chip memory

	2 key issues:
		1) issuing the prefetch at the right time

			-> if too late, then mem values won't be back when the pgm wants to use them and processor has to wait anyway

			-> too early there is a chance the values could be evicted from cache by another need b4 they can be used


		2) issuing the prefetch at the right distance

			-> the "prefetch distance" is how far ahead the prefetch memory is than the memory we are using right now

			-> too far, and the values sit in cache for too long, and possible evicted

			-> too near, and the pgm is ready for the values b4 they have arrived


	To do this, we will need to use the built in function:
		void__builtin_prefetch(void *, int rw, int locality);

		-> random note: there are 2 prefetch registers
 
Parallel Pgm Design Patterns and Strategies
---------------------------------------------------------------------------------------

Functional (or Task) Decomposition Design Pattern
	-> think of sim game where it has climate, animals, plants, and money as tasks



Task Distribution Design for Patterns of ||ism

Could do:

Thread-to-thread (semaphore): all could be sharing data among themselves

Broadcast: 1 thread is main broadcaster that sends data to the other threads

Reduction: already used this.. each thread does something as part of a whole, then aggregates them

Scatter, Gather: found in super computer type work
	-> master thread decomposes the data problem, sends all the pieces out
	-> G collects all the answers back




Decentralized (Peer): input sends to one of any of the "peer threads"
	-> threads something to centralized output


Manager/workers: 
	Mngr input "manager thread"
		-> "worker threads" where the stuff is farmed out to perform
		-> centralized output

Map-Reduce:
	"map thread" input
		-> farms out to worker threads
		-> back into the "accumulate thread"
			-> then to the output


Pipeline: 
	input -> 1st through 4th thread, then to output
		-> like a queue and has a buffer for each thread


Part 2 of DP notes.. (design B)
-----------------------------------------------------------------------
heating simulation:

If we allocate 1 large array, we will see false sharing (and a performance hit) because
all of the data is on the same cache lines, but we are updating that data
	-> we still have to look forwards and backwards, so we will cross the cache line boundaries


If we split it up into arrays for # of cores
	-> split up so each has it's own cache line
	-> T_i-1 and T_i+1 might not be on the same cache line - if you are on the boundary
		-> we need some logic that goes to look this up


Compute to Communicate Ratio

	In our example:

	Intracore computing - we have 4 elements that can all compute amongst each other w/o any need for communication

	Intercore communication: 2 units of communication

		(N is generalized rather than 4)
	Compute: Communicate Ratio = N : 2

			-> 4 core maching C:C = 4:2
			-> 8 core machine C:C = 2:2

				-> "goldilocks and the 3 bears sort of thing"

				-> too small C:C ratio and you are spending all your time sharing data values across threads and doing too little computing

				-> too high ratio, not spreading out your problem among enough threads to get good parallelism

			Difficult to find "sweet spot" w/o running exp's


	2D C:C aka Area-to-Perimeter
		N^2: 4N = N:4 where N is the dimension of compute nodes per core

	3D: vol-to-surface
		N^3: 6N^2 = N:6

		

Week 5
======================================================================================

SIMD - Vector (processing) parallel programming
-----------------------------------------------------------------------------------

Why do we care?
	-> Performance

Many hdw archs today, both CPU and GPU allow you to perform arithmetic ops on multiple array eles simultaneously.
	(Single Instruction Multiple Data)

	EX probs:
		convolution, fourier transform, power spectrum, autocorrelation, etc..

SIMD in Intel chips
1999  SSE  Width(bits) 128   Width (FP words) 4
2013  AVX-512  512   16  (one complete cache line!)   -> xeon phi
		-> prefetch is important here due to this!

		(standard scalar multiplication)
normal assembly ex:		//  inst  src, dst
	mulss  r1, r0     // multiples r contents of each and puts back in r0

SIMD:
	Does this, but 4 of them at the same time
	mulps 	xmm1, xmm0		// mulps src, dst  "att form"
	Think replicating the hardware 3 additional times
		-> x registers (extended)
			-> this is actually 4 floating pt #s, aka 128 bits.
				-> operation is basically the same, but the stuff happens in parallel
				"all 4 xreg1 #s mult into all 4 xreg0 #s"

SIMD Multiplication:

Array * Array

void SimdMul(float *a, float *b, float *c, int len)
{
	c[0:len] = a[0:len] * b[0:len];
} // after : is array size, NOT go through that index


Could do through open mp

Array * Array

void SimdMul(float *a, float *b, float *c, int len)
{
	#pragma omp simd
	for(int i = 0, i < len; i++)
		c[i] = a[i] * b[i];
} 



Array * Scalar
void SimdMul(float *a, float b, float *c, int len)
{
	c[0:len] = a[0:len] * b;
}

void SimdMul(float *a, float b, float *c, int len)
{
	#pragma omp simd
	for(int i = 0, i < len; i++)
		c[i] = a[i] * b;
} 



Combining SIMD w/ || threading in openmp 4.0

#pragma omp parallel for
for(int i = 0; i < ArraySize; i++)
{
	c[i] = a[i] * b[i];
}

+

#pragma omp simd
for(int i = 0; i < ArraySize; i++)
{
	c[i] = a[i] * b[i];
}


have child:

#pragma omp parallel for simd
for(int i = 0; i < ArraySize; i++)
{
	c[i] = a[i] * b[i];
}
	-> this gives the compiler even more license to spread this out on threads, then have the threads use SIMD units to speed it up even more
		-> Prof says he has not found that this produces anywhere near the full potential
			-> but probably will in the future


Requirements for a for-loop to be Vectorized

	i) if there are nested loops, the inner one must be the one vectorized

	ii) no jumps or branches. "masked assignments" (an if-statement controlled assingment are OK)
			-> if (A[i] > 0.)
				 B[i] = 1.;

	iii) total # of iterations must be known at runtime when the loop starts

	iv) cannot be any backward loo dependencies, like this:
			A[i] = A[i-1] + 1.;

	v) helps if the eles have contiguous memory addresses


Prefetching
	-> when whipping through memory this quickly, really impt otherwise cache fetch time becomes a major part of the timing

	-> used to place a cache line in memor b4 it is used, thus hiding the latency of teching from off-chip memory
		-> right time
		-> right distance

What is the catch?
	-> compilers haven't caught up to producing efficient SIMD code
		-> great ways to express the desire for SIMD in code, we won't get the full potential speedup.. yet

		-> for the CPU SIMD project, we are going to investigate the potential speedup using assembly language
			-> given 2 assembly functions

				1) SimdMul: C[0:len] = A[0:len] * B[0:len]

				2) SimdMulSum: return( sum A[0:len] * B[0:len])

					WARNING:
				-> these only work on flip using gcc/g++ w/o -O3



Intel Xeon Phi  (part a)
-----------------------------------------------------------------------------------


Has scalar and vector units
	-> vector registers are 512 bits wide = 16 floats
		-> 64 bytes = 1 cache line.. mutliply an entire cache line at once

Can perform:  Fused Multiply-Add (FMA)

	Scientific or Eng comp take form of: D = A + (B * C)

	normal mult-add would be:
			tmp = B * C
			D = A + tmp

	Fused does this all at once, when the low-order bits of B*C are ready, they are immediately added into the low-order bits of A at the same time the higher-order bits of B * C are being multiplied.






Intel Xeon Phi  (part b)
-----------------------------------------------------------------------------------

Offload mode:

#pragma offload target(mic) in(A:length(NUMS)) in(B:length(NUMS)) out(C:length(NUMS)) out(dtp:length(1))
{
	
	..
	..
}

	in -> data to send over
	out -> data to bring back

	This offloads work to the xeon phi rather than executing on rabbit
		-> takes advantage of all the extra cores



Week 6
======================================================================================

GPU 101
-----------------------------------------------------------------------------------

Originally for graphics, but were powerful so we began using for computing

Why can get some incredible performance from GPUs?
	-> back up to get some gpu history


How did you gain access to GPU power?
	3 ways
		1) write a graphics display program (>= 1985)

		2) write an app that looks like a graphics display pgm, but uses the fragment shader to do some computation (>= 2002)

		3) Write in OpenCL (or CUDA) which looks like C++ (>= 2006)

		graphics is a very parallel thing since we need everything updated at once


Why GPU outpacing CPU performance?

	->Gpu chips ment to stream data
	-> don't need a huge cache since it is jumping around
	-> doesn't need to do branch prediciton
	-> doesn't need the logic to process out-of-order instructions like the CPU does

	b/c they do different things

	Today's GPU devices are less task-specific, so the processing can be repurposed (architecture)


	Looks like 2880 cores/chip.. is this really true?
		Zoom far in is a CUDA Core
			-> FP unit, INT unit.. no processor
			-> GPU core isn't a CPU core.. no pgm counters, can't access stack
				-> it just does arithmetic
		In our case, there is a SM (Streaming Multiprocessor)
			-> 192 CUDA Cores
				-> 192 tasks can be handled at once (computations)

Bottom line:
	-> impossible to directly compare CPU and GPU
		-> optimized to do diff things

CPU
-----

General purpose pgming
Multi-core under user control
Irregular data structures (non-contiguous memory.. like a linked list)
Irregular flow control


GPU
-----

Data parallel pgming
Little user control
regular data structures  (think contiguous memory)
regular flow control



OpenCL equiv of SM = Compute Unit
OpenCL equiv of CUDA Core = Processing Element


GPU Platform can have 1 or more devices
	-> GPU Device is organized as a grid of Compute Units
		-> each compute unit is organized as a grid of Processing Elements


How can GPUs execute General C code efficiently?
	-> do what they do best (don't think about unless you have a very intense data parallel app)

	-> expect thousands of threads, not just a few

	-> each thread executes the same program (called the kernel), but ops on a diff small piece of the overall data

	-> many waking up at the same time, hoping to work on a small piece of the overall problem

	-> opencl has built in fns so that each thread can figure out which thread # it is, thus what part of the overall job it is supposed to do

	-> when thread gets blocked somehow, proc switches to exe'ing another thread to work on



Open CL - A
-----------------------------------------------------------------------------------

Consists of a c/c++ callable api and a c-ish pgm lang
	-> runs on a ton of stuff

-> can share data and interoperate with OpenGL
-> JS implementations of these (such as WebCL)
-> GUP does not have stack, so no recursion and no function calls (no pointers)

Each vendor has a specific driver that compiles the code to their necessary code for their device

How to use?
	-> use normal compiler & linker for your c/c++ code
	-> opencl code c/l on gpu


C/C++:
void ArrayMult( int n, float *a, float *b, float *c)
{
	for ( int i = 0; i < n; i++ )
	c[ i ] = a[ i ] * b[ i ];
}


OpenCL:

kernel
void
ArrayMult( global float *dA, global float *dB, global float *dC)
{
	int gid = get_global_id ( 0 );
	dC[gid] = dA[gid] * dB[gid];
}


supports built in SIMD constructs:
	float4 f, g;


An OpenCL program is organized as a grid of Work-Groups (software vs the hardware name Compute Units)
	-> Each W-G organized as a grid of Work-Items  (sw Processing Element equiv)

	-> each W-G has some local memory and multiple work items to work on
		-> each of which has some private memory

	-> all W-G have a shared global memory



Rules


Threads can:
	-> share mem (local to WG) w/ other threads in same W-G
	-> synchronize w/ other threads in the same WG

Global and Constant memory:
	-> accessible by all threads in all WGs
	-> often cached inside a WG

Each thread:
	-> has registers and private mem

Each WG:
	-> has a max number of registers it can use
		-> div'd equally among all its threads




Open CL - B  (real code!)
-----------------------------------------------------------------------------------

Querying the # of platforms (usually 1)


cl_uint numPlatforms;
status = clGetPlatformIDs( 0, NULL, &numPlatforms );
if( status != CL_SUCCESS )
	fprintf( stderr, "clGetPlatformIDs failed (1)\n" );

fprintf( stderr, "Number of Platforms = %d\n", numPlatforms );

cl_platform_id * platforms = new cl_platform_id[ numPlatforms ];
status = clGetPlatformIDs( numPlatforms, platforms, NULL );
if( status != CL_SUCCESS )
	fprintf( stderr, "clGetPlatformIDs failed (2)\n" );

^ this might be part of the code from printinfo

				How many to get, where to put them, how many total there are
status = clGetPlatformIDs( 0, NULL, &numPlatforms );	// how many do we have?
status = clGetPlatformIDs( numPlatforms, platforms, NULL );		// go get the info


Grab the code from class website to print the CL error to make our lives easier.
	errorcodes.cpp.. grabbed and saved to pc


// find out how many devices are attached to each platform and get their ids:

status = clGetDeviceIDs( platform, CL_DEVICE_TYPE_ALL, 0, NULL, &numDevices );

devices = new cl_device_id[ numDevices ];

status = clGetDeviceIDs( platform, CL_DEVICE_TYPE_ALL, numDevices, devices, NULL );



// just getting the gpu device:

cl_device_id device;
status = clGetDeviceIDs( platform, CL_DEVICE_TYPE_GPU, 1, &device, NULL );




Query to see what extensions are supported on this device:

size_t extensionSize;

clGetDeviceInfo( device, CL_DEVICE_EXTENSIONS, 0, NULL, &extensionSize );
char *extensions = new char [extensionSize];
clGetDeviceInfo( devices, CL_DEVICE_EXTENSIONS, extensionSize, extensions, NULL );

fprintf( stderr, "\nDevice Extensions:\n" );
for( int i = 0; i < (int)strlen(extensions); i++ )
{
	if( extensions[ i ] == ' ' )
		extensions[ i ] = '\n';
}
fprintf( stderr, "%s\n", extensions );
delete [ ] extensions;

Extensions we care about:
	cl_khr_gl_sharing  // this is the big one we ar elooking for. OpenCL can interoperate with OpenGL

	cl_khr_fp64 	// handy as well.  OpenCL system can support 64-bit floating point (i.e. double precision)






Open CL - C
-----------------------------------------------------------------------------------

Steps in creating and running an open cl program

1. Program header
2. Allocate the host memory buffers
3. Create an OpenCL context
4. Create an OpenCL command queue
5. Allocate the device memory buffers
6. Write the data from the host buffers to the device buffers
7. Read the kernel code from a file
8. Compile and link the kernel code
9. Create the kernel object
10.Setup the arguments to the kernel object
11.Enqueue the kernel object for execution
12.Read the results buffer back from the device to the host
13.Clean everything up



#include "cl.h"  // for open CL   (step 1)


// allocate the host memory buffers:  (step 2)

float * hA = new float [ NUM_ELEMENTS ];
float * hB = new float [ NUM_ELEMENTS ];
float * hC = new float [ NUM_ELEMENTS ];

// the above could have also been done: float hA[ NUM_ELEMENTS ];
	// global mem and the heap typical have lots more space than the stack, so we don't want
	// to allocate a large array like this as a local variable



// fill the host memory buffers:

for( int i = 0; i < NUM_ELEMENTS; i++ )
{
	hA[ i ] = hB[ i ] = sqrtf( (float) i );
}

// array size in bytes (will need this later):

size_t dataSize = NUM_ELEMENTS * sizeof( float );

// opencl function return status:

cl_int status; 		// test against CL_SUCCESS




// create an open cl context (step 3)

// create a context:

cl_context context = clCreateContext( NULL, 1, &device, NULL, NULL, &status );

	// properties, one device, this device, callback, pass in user data, returned status




// create a command queue:  (step 4)

cl_command_queue cmdQueue = clCreateCommandQueue( context, device, 0, &status );
	
	// the context, the device, properties, returned status



// allocate memory buffers on the device: 	(step 5)

cl_mem dA = clCreateBuffer( context, CL_MEM_READ_ONLY, dataSize, NULL, &status );
cl_mem dB = clCreateBuffer( context, CL_MEM_READ_ONLY, dataSize, NULL, &status );
cl_mem dC = clCreateBuffer( context, CL_MEM_WRITE_ONLY, dataSize, NULL, &status );

	// context, how this buffer is restricted, # bytes, buffer data already allocated, returned status


	-> read and write terminology wrt the OpenCL device
		-> CL_MEM_READ_ONLY means that the OCL device can only get this data
			-> can't send it back to the host CPU
		-> other options
			-> CL_MEM_WRITE_ONLY
			-> CL_MEM_READ_WRITE



write that data from the host buffers to the device buffers

// enqueue the 2 commands to write data into the device buffers: (step 6)

status = clEnqueueWriteBuffer( cmdQueue, dA, CL_FALSE, 0, dataSize, hA, 0, NULL, NULL );
status = clEnqueueWriteBuffer( cmdQueue, dB, CL_FALSE, 0, dataSize, hB, 0, NULL, NULL );

	// command queue, device buffer, want to block until done?, offset, # bytes, host buffer, # events, event wait list, event object


Open CL - D
-----------------------------------------------------------------------------------


kernel		// can be fired off from the C program
void  				// local gpu addrs of these parms
ArrayMult( global const float *dA, global const float *dB, global float *dC )
{
	int gid = get_global_id( 0 );   // gid = which ele we are dealing w/ right now
						// which dim's index are we fetching? 
						// 0 = X, 1 = Y, 2 = Z
						// 1D prob, only doing X

	dC[gid] = dA[gid] * dB[gid];
}





// read the kernel code from a file into a char array  (step 7)

const char *CL_FILE_NAME = { “arraymult.cl" };
. . .

FILE *fp = fopen( CL_FILE_NAME, "r" );  // "r" shoudl work since the .cl file is pure ASCII text
			// but some ppl report that it doesn't work unless you use "rb"
			// watch out for the '\r' + '\n' problem!... see below
if( fp == NULL )
{
	fprintf( stderr, "Cannot open OpenCL source file '%s'\n", CL_FILE_NAME );
	return 1;
}

// read the characters from the opencl kernel program:

fseek( fp, 0, SEEK_END );
size_t fileSize = ftell( fp );		// where are we (how many bytes in)
fseek( fp, 0, SEEK_SET );
char *clProgramText = new char[ fileSize+1 ]; 		// allocate big long string
size_t n = fread( clProgramText, 1, fileSize, fp );  // read stuff into it
clProgramText[fileSize] = '\0';
fclose( fp );



.cl on windows
	-> have strong unexplainable problems with sch scripts, or cpp/cl pgms
		-> windows inserts \r for carriage return


		test:
			od -c loop.csh

			get rid of c rtns:
				tr -d '\r' < loop.csh > loop1.csh
					-> then run loop1.csh

			on some systems:

			dox2unix < loop.csh > loop1.csh


compile and link kernel code

// create the kernel program on the device:   (step 8)

char * strings [ 1 ];  		// an array of strings
strings[0] = clProgramText;
cl_program program = clCreateProgramWithSource( context, 1, (const char **)strings, NULL, &status );
delete [ ] clProgramText;


// build the kernel program on the device:

char *options = { "" };
status = clBuildProgram( program, 1, &device, options, NULL, NULL );
if( status != CL_SUCCESS )
{  				// retrieve and print the error messages:
	size_t size;
	clGetProgramBuildInfo( program, devices[0], CL_PROGRAM_BUILD_LOG, 0, NULL, &size );
	cl_char *log = new cl_char[ size ];
	clGetProgramBuildInfo( program, devices[0], CL_PROGRAM_BUILD_LOG, size, log, NULL );
	fprintf( stderr, "clBuildProgram failed:\n%s\n", log );
	delete [ ] log;
}





Open CL - E
-----------------------------------------------------------------------------------

// create the kernel object  (step 9)

cl_kernel kernel = clCreateKernel( program, “ArrayMult", &status );


// setup the args to the kernel obj  (step 10)

		// kernel, arg #, cl_mem ptr, address
status = clSetKernelArg( kernel, 0, sizeof(cl_mem), &dA );
status = clSetKernelArg( kernel, 1, sizeof(cl_mem), &dB );
status = clSetKernelArg( kernel, 2, sizeof(cl_mem), &dC );
		// these are from ArrayMult function where the parms were ptrs




// enqueue the kernel obj for execution   (step 11)

size_t globalWorkSize[ 3 ] = { NUM_ELEMENT, 1, 1 };
size_t localWorkSize[ 3 ] = { LOCAL_SIZE, 1, 1 } ;

status = clEnqueueBarrier( cmdQueue );

double time0 = omp_get_wtime( );

status = clEnqueueNDRangeKernel( cmdQueue, kernel, 1, NULL, globalWorkSize, localWorkSize, 0, NULL, NULL );
		// cmd queue, kernel, # dimensions, global work offset (always null), global work size, local work size, # events, event wait list, event object

status = clEnqueueBarrier( cmdQueue );

double time1 = omp_get_wtime( );



Work groups, local ids, and global ids
	# WGs = GlobalIndexSpaceSize / WorkGroupSize



Figure out what thread you are and what your thread environment is like:

uint get_work_dim( ) ;
size_t get_global_size( uint dimindx ) ;
size_t get_global_id( uint dimindx ) ;  		// we have seen this one
size_t get_local_size( uint dimindx ) ;		// args 0, 1, or 2
size_t get_local_id( uint dimindx ) ;
size_t get_num_groups( uint dimindx ) ;
size_t get_group_id( uint dimindx ) ;
size_t get_global_offset( uint dimindx ) ;



// read the results buffer back from the device to the host   (step 12)

	NOTE: don't do this right away w/o a barrier or something since it reads back right away

status = clEnqueueReadBuffer( cmdQueue, dC, CL_TRUE, 0, dataSize, hC, 0, NULL, NULL );
	// command queue, device buffer, want to block until done, offset, # bytes, host buffer, # events, event wait list, event object




// clean everything up:    (step 13)

clReleaseKernel( kernel );
clReleaseProgram( program );
clReleaseCommandQueue( cmdQueue );
clReleaseMemObject( dA );
clReleaseMemObject( dB );
clReleaseMemObject( dC );

delete [ ] hA;
delete [ ] hB;
delete [ ] hC;




writing the .cl pgms binary code:

size_t binary_sizes;
status = clGetProgramInfo( Program, CL_PROGRAM_BINARY_SIZES, 0, NULL, &binary_sizes );

size_t size;
status = clGetProgramInfo( Program, CL_PROGRAM_BINARY_SIZES, sizeof(size_t), &size, NULL );

unsigned char *binary = new unsigned char [ size ];
status = clGetProgramInfo( Program, CL_PROGRAM_BINARIES, size, &binary, NULL );

FILE *fpbin = fopen( "particles.nv", "wb" );
if( fpbin == NULL )
{
	fprintf( stderr, "Cannot create 'particles.bin'\n" );
}
else
{
	fwrite( binary, 1, size, fpbin );
	fclose( fpbin );
}
delete [ ] binary;

// you have to import the binary code back in at step 8 though..


PROJECT 4 NOTES
---------------------------------------------------------------------------------

video
----------------------------------

month by month grain-growing simulation
	-> grain affected by temp, precip, # of "graindeer" to eat it

	-> given "states" and basic one month parms


3 things going
	i) deer
	ii) grain
	iii) watcher thread  (determine rain, temp, printing stuff)
	iv) prj asks for 4th quantity
		-> interacts with deer and grain
		-> recommended to get sim working w/o it first
		-> we know number of threads ahead of time (3 or 4)
			-> good use for sections!
				Each section can have a function which you call the thing you are monitoring
					-> Graindeer, Grain, Watcher, MyAgent

	Make sure to watch your barriers when updating the states, then printing states

Quantity interactions:
	i) if # graindeer exceeds the number of inches of height of the grain at the end of the month, decrease the graindeer by 1

	ii) if # grainder is less than this value at EOM, increase GD by 1


use rand_r()
	-> not rand()


use cm and degrees C to make everything look cleaner
	cm = inches * 2.54
	C = (5/9.)*(F-32)




text
----------------------------------

Given some functions to rtn a random number 



#include <stdlib.h>

unsigned int seed = 0;  // a thread-private variable
float x = Ranf( &seed;, -1.f, 1.f );

. . .

float
Ranf( unsigned int *seedp,  float low, float high )
{
        float r = (float) rand_r( seedp );              // 0 - RAND_MAX

        return(   low  +  r * ( high - low ) / (float)RAND_MAX   );
}


int
Ranf( unsigned int *seedp, int ilow, int ihigh )
{
        float low = (float)ilow;
        float high = (float)ihigh + 0.9999f;

        return (int)(  Ranf(seedp, low,high) );
}




Turn in your code and your PDF writeup. Be sure your PDF is a file all by itself, that is, not part of any zip file. Your writeup will consist of:

 

1)  What your own-choice quantity was and how it fits into the simulation.
 

2)  A table showing values for temperature, precipitation, number of graindeer, height of the grain, and your own-choice quantity as a function of month number.
 

3)  A graph showing temperature, precipitation, number of graindeer, height of the grain, and your own-choice quantity as a function of month number. Note: if you change the units to °C and centimeters, the quantities might fit better on the same set of axes.

cm = inches * 2.54 
°C = (5./9.)*(°F-32)

This will make your heights have larger numbers and your temperatures have smaller numbers.

 

4)  A commentary about the patterns in the graph and why they turned out that way. What evidence in the curves proves that your own quantity is actually affecting the simulation?






Week 7
======================================================================================


-----------------------------------------------------------------------------------















