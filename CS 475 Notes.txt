CS 475 Notes
Spring 2018


Project Notes
================================================================================


double time0 = omp_get_wtime();
	-> OpenMP call
	-> used to get time



USE SCRIPTING TO MAKE THIS SHIT EASIER TO TEST DIFFERENT NUMBERS!


Simple OpenMP intro
=============================================================================

Open Multi-Processing

	-> mostly compiler directives "pragmas"

#pragma omp directive [clause]

turn it on in Linux:
	g++ -o proj proj.cpp -lm -fopenmp

For windows turn on, see slide 4 of the lecture
	-> in config props in VS


Coolest thing (possibly):


	omp_set_num_threads(NUMT);  // sets how many threads will be in the thread pool

	...

	#pragma omp parallel for
	for(int i = 0; i < arraySize; i++)
	{....}

	-> code starts as a single thread,
	then this uses NUMT threads to execute the loop simultaneously



Parallel programming background infomation

Lecture 1 (A?)
=======================================================

3 reasons:
	1) increase performance (more work same time)
	2) increase performance (take lest time to do the same amt of work)
	3) make some programming tasks more convenient to implement


3 types of parallelism:

	1) instruction level parallelism (ILP)
		-> out of order execution
			-> logic looks ahead to grab some code that it can execute while waiting on something that might be blocked (such as fetchign something that isn't in cache (getting from memory))

			-> if compiler does this = Static ILP
			-> if CPU chip = Dynamic ILP

		Note: we can't control this


	2) Data Level Parallelism (DLP)
		-> executing the same instructions on different parts of the data
			If we have a for loop on a large array and same instructions.. we can break this up!
				-> multiple threads on multiple cores

			We will use OpenMP with this!


	3) Thread Level Parallelism (TLP)
		-> executing DIFFERENT instructions
			Ex: variety of incoming transaction requests
		-> implies you have more threads than cores
		-> thread exec switches when a thread blocks or uses up its time slice

State:
	-> Registers
	-> Program Counter
	-> Stack Pointer

	(We will visit this again in multi-core programming)


Process:
	executes a program in memory

	keeps a state (see above)


Thread:
	separate indp procs, all execing a common pgm and sharing memory
		-> each thread has its own state

	-> only thing different is each thread has it's own stack

	can time share on a single processors
		(don't need multiple processors, but you can - MULTICORE COMING SOON!)



lecture 2
=============================================================================

Conflicts in multithreaded programs.. Thread safety
	-> can take a function that be used many times and won't alter the state of something

	What is not thread safe?

		Ex: char *strtok (char * str, const char * delims);

		-> this keeps internal state info.. if we alter that state in a 2nd thread, this screws with the first threads info.

		strtok_r returns the internal state (new version of strtok)
			-> so you can pass back when ready.. 'r' stands for "re-entrant"



Deadlock Fault Problems:
	Deadlock: 2 threads are each waiting for the other to do something


Race Condition Fault:
	-> where it matters which trhead gets to a particular piece of code first
	-> often comes about when on thread is modifying a var while the other thread is in the midst of using it

	Often fixed with Mutual Exclusion Locks (Mutexes)
		-> lock the stuff while we do something, then unlock and let the other grab it and do its thing

volatile keyword: used to let the compiler know that another thread might be changing a variable "in the background", so don't make any assumptions about what can be optimized away.
	Ex:  volatile int val = 0;
		w/o volatile a good compiler would know that val == 0



restrict keyword: w/o this could create race condition in the "optimized", but wrong code.
	-> this is a "promise" that the vars will never point to the same memory location


lecture 3
=======================================================================================

Functional (or Task) Decomposition:
	-> breaking a task into sub-tasks that represent separate fns
	-> primarily indpt


Domain (or Data) Decomposition
	-> breaking task into sub-tasks that represent separate sections of data
	Ex: big matrix solutions


Atomic: An operation that takes place to completion w/ no chance of being interrupted by another thread

Deterministic: same set of inputs always gives the same outputs

Reduction: Combining the results from multiple threads into a single sum or product, continuing to use multithreading. Typically performed so that it takes O(log_2 N) time instead of O(N) time

Fine-grained parallelism: breaking a task into lots of small tasks

Coarse-grained parallelism: breaking a task up into small number of large tasks

Barrier: A point in the pgm where ALL threads must reach before any of them are allowed to proceed


Fork-join: operationg whr multiple threads are created from a main thread.  All of those forked threads are expected to eventually finish and thus "join back up" w/ the main thread

Shared Variable: After a fork operation, a var which is shared among threads, i.e., has a single value

Private variable: After a fork op, a var which has a private copy w/in each thread

Static scheduling: dividng the total # of tasks T up so that each of N available threads has T/N sub-tasks to do

Dynamic scheduling: dividing the total # of tasks T up so that each of N available threads has less than T/N sub-tasks to do, and then doling out the remaining tasks to threads as they become available

Speed-up(N)   T_1 / T_N

Speed-up Efficiency Speed-up(N) / N






WEEK 2
---------------------------------------------------------------------------------
(lecture 4 was week 2 welcome)

lecture 5 - Open MP A
=================================================================================

both directive and library-based

shared executable, global memory, and heap

Not totally deterministic, so no guarantee identical behavior accross venders or hardware
	-> even multiple runs on same hardware

Doesn't guarantee order of execution either



Open MP usage:
g++ -o proj proj.cpp -lm -fopenmp

for intel optimization:
icpc -o proj proj.cpp -lm -fopenmp -align -qopt-report=3 -qopt-report-phase=vec


set num threads:
omp_set_num_threads(num);

ask how many cores this pgm has access to:
num = omp_get_num_procs();

how many threads using right now?:
num = omp_get_num_threads();

which thread one is?
me = omp_get_thread_num();



#pragma omp parallel default(none)
	-> creates the team of threads to exec
	-> threads won't be in order


lecture 6 - Open MP B
=========================================================================================


#pragma omp parallel for default(none)
	-> creates team of threads and divides them among the for loop passes w/ 1 line of code
	-> default(none) - could remove the implied barrier with this optional item

	default(none) aka no default
		-> ensures variables are explicitly declared inside and outside the threaded portion w/ shared or private

	private(x) - means each thread has its own copy of var x

	shared(x) - all threads shared a common x


	Ex:
	#pragma omp parallel for default(none), private(i,j), shared(x)


if you don't schedule with OpenMP, it defaults to static
	schedule(static [,chunksize])
	schedule(dynamic [,chunksize])
			-> chunksize defaults to 1


lecture 6 - Open MP C
=========================================================================================


Where can we see problems?

	Ex: adding a sum in the for loop that has be made to operate in parallel

		-> no guarantee the sum line is exec'd correctly

		-> no guarantee each thread will finish this line b4 some other thread interrupts it
			multiple Assembly code lines are usually is generated

		-> non-deterministic


	3 solutions:

		1)   #pragma omp atomic
			-> fixes non-deterministic problem
			-> tries to use a built in hardware instruction

		2)  #pragma omp critical
			-> disables scheduler interrupts during the critical section


				BEST METHOD
		3) #pragma omp parallel for reduction(+:sum),private(myPartialSum)
			....
			sum = sum + myPartialSum;

			-> in O(log_2 N) instead of O(N)
			-> reduction creates a sum private var for each thread
				-> adds each some in a binary fashion (hence O(log_2 N))



lecture 7 - Open MP D
===========================================================================================

Synchronization:

	Mutual Exclusion Locks (Mutexes)

		omp_init_lock(omp_lock_t *);
		omp_set_lock(omp_lock_t *);  // blocks if lock not available, sets and rtns when avail
		omp_unset_lock(omp_lock_t *);
		omp_test_lock(omp_lock_t *);  // if lock not avail, rtn 0, if avail, sets and retrns !0

Critical sections
	#pragma omp critical
		-> restricts exec to 1 thread at a time

	#pragma omp single
		-> restricts exe to a single thread ever

Barriers

	#pragma omp barrier
		-> forces each thread to wait here until all threads arrive
			-> implied w/ for loops and openMP sections, unless NOWAIT clause is used.


Creating sections of MP Code
	-> indp blocks of code, able to be assigned to separate threads if they are available

	#pragma omp parallel sections
	{
		#pragma omp section
		{
			Task 1
		}

		#pragma omp section
		{
			Task 2
		}
	}  // implied barrier here



OpenMP Tasks - works like a section, but created on the fly

	#pragma omp task
		code here

	Force a barrier
		#pragma omp taskwait


lecture 9 - Speedups and Amdahl's law
===========================================================================

Using n processors, speedup_n is:

	speedup_n = T1 / Tn... actually, I think this should be flipped with Tn on top. as speedup should be > 1
						-> if you take the inverse of this number, you get something > 1
							-> which is just the number of Tn / T1 anyway.

Speedup Efficiency_n:
		E_n = Speedup_n / n  (could be as high as 1)




Amdahl's Law:
	-> mainly used to predict the theoretical max speedup of a program processing using multiple processors

	Speedup_n = T1 / Tn
		= 1 /  [ (F_parallel / n) + F_sequential ]

		= 1 / [ (F_p / n) + (1 - F_p)]


			-> parallel potion can be cut down, but not sequential

	-> if we know the max F_p, we can calc the max speedup


	F_p =   n / (n-1)   * T1 - Tn / T1
		=  n / (n-1)  * ( 1 -  1 / speedup)



	maxspeedup = 1 / 1 - F_p



Gustafson's Obersvation:
	As amount of data increases, you can increase the parallel fraction
		-> data generally comes in on the parallel side rather than the sequential side
			-> it does still have to be read in, so there is some sequential, so this is somewhat theoretical

lecture 10 - Moore's Law and Multicore
===========================================================================

Moore's Law
	-> obersvation that "transistor density doubles every 1.5 years"
		-> does not necessarily mean clock speed does the same.. was a artifact of the above

After about 2003 - transistor density is still following the law, but clock speed and power cosnumption leveled off

Power Consumption (by the chip) is proportional to ClockSpeed ^ 2
	-> if you keep doubling clock speed, this would get incredibly hot
		For instance: by about mid 00's, it would be as hot as a nuclear reactor
			-> 2015 ish = sun's surface



Since we aren't increasing clock speed due to power consumption (hitting a wall), what do we do with the extra space?
	-> pack multiple processors onto the same chip
		-> this is MultiCore

MultiCore and Multithreading

MC w/o MT is a good thing.  Use to allow multiple pgms on a desktop system to always be executing concurrently.  (MC is basically more hardware aka processors)

MT w/o MC, still a good thing.  Threads can make it easier to logically have many things going on in your program at a time, and can absorb the dead-time of other threads.
	-> Largely only software

Big performance gain: use both to speed up a single program.
	-> we need to be prepared to convert our programs to run on MultiThreaded Shared Memory Multicore architectures



HT = Hyper Threading
	-> processor could have HT, but not multiple execution units
	-> not really 2 simul threads, but these 2 can swap quickly
	-> most important: count the execution units



week 3
------------------------------------------------------------------------------------

bubble sort
================================================================================

If we remove the inner for loop to see what is happening with bubble sort and parallelizing it, we basically have a bunch of conditionals inside 1 loop
	-> 1 conditional could be on one thread and the next on a different.
		-> since we have no control over thread scheduling, this is a race condition

	We would then do the pragma omp statement on the inner loop:
		#pragma omp parallel for default(none),shared(A, first)

		take note that the inner loop jumps by 2 (this looks at pairs)
			-> outer loop starts with odd or even based on first

for( int i = 0; i < N; i++ )
{
	int first = i % 2; 		// 0 if i is 0, 2, 4, ...
							// 1 if i is 1, 3, 5, ...

	#pragma omp parallel for default(none),shared(A,first)

	for( int j = first; j < N-1; j += 2 )
	{
		if( A[ j ] > A[ j+1 ] )
		{
			std::swap( A[ j ], A[ j+1 ] );
		}
	}
}


tree traversal using openmp tasks
===============================================================================
Tasks are good for DS that you don't know how big they are (LL, Trees, etc..)

Assume we want to traverse as quickly as possible
	-> BUT DO NOT CARE ABOUT ORDER


	#pragma omp parallel
	{
		#pragma omp single

		Traverse(root); // w/o this, each thread does a full traversal
						// at best = wasteful
						// at worst = dangerous
	}

	#pragma omp taskwait  // if you want to wait for all nodes to be traversed b4 proceeding


void
Traverse( Node *n )
{
	if( n->left != NULL )
	{						// with tasks, these types of things are added to a task's to do list
		#pragma omp task private(n) untied   // untied - means another open thread can steal 											this one (to work on it)
		Traverse( n->left );
	}

	if( n->right != NULL )
	{
		#pragma omp task private(n) untied
		Traverse( n->right );
	}

	#pragma omp taskwait	// if you want both branches to be taken b4 processing the parent
	// can get rid of this if we don't care as it will slow stuff down

	Process( n );
}

Summary:

Tasks get spread among the current "thread team"

Tasks can execute immediately or be deferred.  They are exec'd at "some time."

Tasks can be moved b/w threads, that is, if one thread has a backlog of tasks to do, an idel thread can come steal some workload.

Tasks are more dynamic than sections.  The task paradigm would still work if there was a variable number of children at each node.



cache A
============================================================================

Problem: the path b/w a cpu chip and off-chip memory is slow
	-> put in "intermediate memory"
		-> Hierarchical Memory systems or "Cache"

	-> cache and memory are named by "distance level" from the ALU
		L1 - smaller, faster, closer
		L2 - slightly larger, a little slower, a little further away

		-> cache brings more things in from memory so it doesn't have to go as far to get it


Example of characteristics:
	L1 cache: 32KB + 32KB  .. or Instruction and Data cache



Cache hit: when cpu asks for a value from memory and that value is already in the cache, it can get it quickly

Cache miss: cpu asks for val in mem, val not in cache, it will have to go off the chip to get it

Cache Line: while cache might be multiple kilo or megabytes, the bytes are transferred in much smaller quantities (each is called this)
	-> size of a cache line is typically just 64 bytes



Successful cache use depends on both:

Spatial Coherence:
	“If you need one memory address’s contents now, then you will probably
	also need the contents of some of the memory locations around it soon.”

Temporal Coherence:
	“If you need one memory address’s contents now, then you will probably
	also need its contents again soon.”


EX:
	If have 2 for loops (1 nested) and we are accessing rows and columns, think of a 4 x 4 grid of values.
		-> If you access the first 16 positions, this will be fast due to the values being stored in cache (across the rows)
		-> if you go down via columns, this is slower since not all the values are in cache
			-> we are accessing things out of order



cache B
==============================================================================

Which is better: array of structs or structure of arrays?

	-> if we are going to use them in groups (x, y, z), then the array of structs works well
		-> better spatial coherence

	-> if we want to take the average of numbers where we add up all the arrays
		-> better to use structure of arrays (list)


PREVIEW FOR SIMD:
	Dx[0:N] = X[0:N] - Xnow;  // a for loop from 0 to N-1



Good OOP style can be incosistent with Good Cache use
	-> new/malloc could be all over the place.. bad for spatial coherence

		-> to solve, you use new/malloc selectively
			-> don't malloc 1 at a time, do a bunch so we get a bunch of memory addresses next to each other
				-> when you add a new list element, you grab it from the ones you created

				*great spatial coherence*


Why can we get decreased performance as data sets get larger?
	-> violating temporal coherence
		-> only using every value once, so it is in cache but we never use it again
		-> arthimatic is outrunning your ability to reload

Preview of Pre-Fetching (later in SIMD)
	-> helps with temporal problem

	-> when you are in a for loop and load the 0th ele, pre-fetch the 16th

Example of where cache coherence matters:
	Matrix multiplication
		-> goes across Row of A, but down col of B

	*You don't have to do this in i, j, k order*
		-> if you did something like i, k, j
			-> goes row i of A and row k of B
				-> produces ele (i, j) of C

				*i k j has the best spatial coherence*


cache C
===============================================================================

cache architectures

N-way Set Associate - cache line from a particular block of memory can appear in a limited number of places in a cache.  Each "limited place" is called a set of cache lines  A set contains N cache lines.

	N is typically 4 for L1 and 8 or 16 for L2


64 bytes in a cache line (refresher)


Each core has its own separate L2 cache
	-> say 2 cores are operating on vars in your pgm that happen to live on the same cache line
		-> fine if they read
		-> if one writes, the other is no longer correct.

		So cache maintains 4 states
			1. Modified
			2. Exclusive
			3. Shared
			4. Invalid

			(MESI)


			Say you do this:
				1) read something with core A..
						A = exclusive

				2) core B reads val from same mem area
						A = shared
						B = shared

				3) B writes into that val, retagged as mod'd, core A re-tagged
						A = Invalid
						B = Modified

				4) A tries to read val from same part of mem, but this is invalid
					-> core B cache line has to be flushed back to mem
					-> then A cache line is re-loaded from mem
					-> now both are shared

					This is False Sharing - huge performance hit (not incorrect results)


cache D
==================================================================================


False Sharing Example Problem:

struct s
{
	float value;
	int pad[NUMPAD];  // we could set numpad=3, which would space it out enough to not 								invalidate the cache line
					// can be variable from 0 to 15 (0 basically gotten rid of by compiler)
					// setting to 7, we have 2 on each of the 1st 2 cache lines
					// setting to 15, we have 1 on each of 4 cache lines, thus best performance
} Array[4];

omp_set_num_threads( 4 );

#pragma omp parallel for
	for( int i = 0; i < 4; i++ )
	{
		for( int j = 0; j < SomeBigNumber; j++ )
		{
			Array[ i ].value = Array[ i ].value + (float)rand( );
			// this reads and writes to the same, thus invalidating the cache line
		}
	}

	* 1 kind of fix is the above... this is to pad your data structures to put key values on different cache lines.. this optimizes the efficience.. *
		-> by wasting memory, you can improve performance.. who'da thunk it
			-> "being cache aware"


Potential fix #2:

If we use local vars, instead of continguous array locations, that will spread out our writes out in memory, and to different cache lines

	-> temp local variables live on each thread's stack
		-> thus not next to each other in memory





#include <stdlib.h>
struct s
{
	float value;
} Array[4];

omp_set_num_threads( 4 );

const int SomeBigNumber = 100000000;

#pragma omp parallel for
	for( int i = 0; i < 4; i++ )
	{
		float tmp = Array[ i ].value;  // THIS GUY.. makes this a private variable that lives in 									each thread's individual stack
							// this works b/c a localized temp variable is created in each core's stack area, so little or no cache line conflict exists

		for( int j = 0; j < SomeBigNumber; j++ )
		{
			tmp = tmp + (float)rand( );
		}
		Array[ i ].value = tmp;
	}

		*this does one read and one store per exe *


malloc'ing on a cache line
	-> know each cache line starts on a fixed 64-byte doundaries lets you do this:
		top N-6 bits tell you what cache line number this address is a part of.
		btm 6 bits tell you what offset that address has w/in that cache line.

			Ex: 32-bit mem system:

			|	32-6 = 26 bits     |    6 bits |
				cache line number 		offset in that cache line

				*so, if you see a mem add whose btm 6 bits are 000000, then you know that that mem location begins a cache line*


Normal allocation of a struct where you want to mallloc an ARRAYSIZE array of them:

struct xyzw *p = (struct xyzw *) malloc( (ARRAYSIZE)*sizeof(struct xyzw) );
struct xyzw *Array = &p[0];
. . .
Array[ i ].x = 10. ;



Want to make sure that array of structs started on a cache line boundary:

unsigned char *p = (unsigned char *) malloc( (ARRAYSIZE+1)*sizeof(struct xyzw) );
int offset = (int)p & 0x3f; 						// 0x3f = bottom 6 bits are all 1’s
struct xyzw *Array = (struct xyzw *) &p[64-offset];
. . .
Array[ i ].x = 10. ;


To free, use free(p);
Not free(Array);




Week 4
======================================================================================

rabbit lecture
-----------------------------------------------------------------------------------

rabbit.engr.oregonstate.edu
	-> used explicitely for this class that we can ssh into
	-> xeon system, 2 processor, 16 cores total, 64 gb of memory

	-> attached to it is 15SMs and 2800 CUDA cores 6GB graphics card
	-> attached xeon phi 57 core system
		-> 4 way hyper threaded (224 hyperthreads)


	lscpu -> gives info about cpu


	Login:   rabbit.engr.oregonstate.edu -l yourengrusername
									^ dash el

		-> get your stuff working first, then go here

		Need to put stuff in rabbit act's .cshrc 
			-> source .cshrc

			Check out slide 7 for this week on what you need to enter.

	NOT REQUIRED TO RUN ANY TESTS ON HERE, BUT GOOD FOR INTERESTING DATA AND FUN


Prefetching lecture
--------------------------------------------------------------------------------

Prefetching -> the process go going out and reaching ahead and saying " I don't need it yet, but sometime pretty soon I'm going to need this cache line out here ahead"
	-> compiler specific

	-> used to place cache line in memory before it is to be used, thus hiding the latency of fetching from off-chip memory

	2 key issues:
		1) issuing the prefetch at the right time

			-> if too late, then mem values won't be back when the pgm wants to use them and processor has to wait anyway

			-> too early there is a chance the values could be evicted from cache by another need b4 they can be used


		2) issuing the prefetch at the right distance

			-> the "prefetch distance" is how far ahead the prefetch memory is than the memory we are using right now

			-> too far, and the values sit in cache for too long, and possible evicted

			-> too near, and the pgm is ready for the values b4 they have arrived


	To do this, we will need to use the built in function:
		void__builtin_prefetch(void *, int rw, int locality);

		-> random note: there are 2 prefetch registers
 
Parallel Pgm Design Patterns and Strategies
---------------------------------------------------------------------------------------

Functional (or Task) Decomposition Design Pattern
	-> think of sim game where it has climate, animals, plants, and money as tasks



Task Distribution Design for Patterns of ||ism

Could do:

Thread-to-thread (semaphore): all could be sharing data among themselves

Broadcast: 1 thread is main broadcaster that sends data to the other threads

Reduction: already used this.. each thread does something as part of a whole, then aggregates them

Scatter, Gather: found in super computer type work
	-> master thread decomposes the data problem, sends all the pieces out
	-> G collects all the answers back




Decentralized (Peer): input sends to one of any of the "peer threads"
	-> threads something to centralized output


Manager/workers: 
	Mngr input "manager thread"
		-> "worker threads" where the stuff is farmed out to perform
		-> centralized output

Map-Reduce:
	"map thread" input
		-> farms out to worker threads
		-> back into the "accumulate thread"
			-> then to the output


Pipeline: 
	input -> 1st through 4th thread, then to output
		-> like a queue and has a buffer for each thread


Part 2 of DP notes.. (design B)
-----------------------------------------------------------------------
heating simulation:

If we allocate 1 large array, we will see false sharing (and a performance hit) because
all of the data is on the same cache lines, but we are updating that data
	-> we still have to look forwards and backwards, so we will cross the cache line boundaries


If we split it up into arrays for # of cores
	-> split up so each has it's own cache line
	-> T_i-1 and T_i+1 might not be on the same cache line - if you are on the boundary
		-> we need some logic that goes to look this up


Compute to Communicate Ratio

	In our example:

	Intracore computing - we have 4 elements that can all compute amongst each other w/o any need for communication

	Intercore communication: 2 units of communication

		(N is generalized rather than 4)
	Compute: Communicate Ratio = N : 2

			-> 4 core maching C:C = 4:2
			-> 8 core machine C:C = 2:2

				-> "goldilocks and the 3 bears sort of thing"

				-> too small C:C ratio and you are spending all your time sharing data values across threads and doing too little computing

				-> too high ratio, not spreading out your problem among enough threads to get good parallelism

			Difficult to find "sweet spot" w/o running exp's


	2D C:C aka Area-to-Perimeter
		N^2: 4N = N:4 where N is the dimension of compute nodes per core

	3D: vol-to-surface
		N^3: 6N^2 = N:6

		
























