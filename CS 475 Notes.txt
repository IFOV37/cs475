CS 475 Notes
Spring 2018


Project Notes
================================================================================


double time0 = omp_get_wtime();
	-> OpenMP call
	-> used to get time



USE SCRIPTING TO MAKE THIS SHIT EASIER TO TEST DIFFERENT NUMBERS!


Simple OpenMP intro
=============================================================================

Open Multi-Processing

	-> mostly compiler directives "pragmas"

#pragma omp directive [clause]

turn it on in Linux:
	g++ -o proj proj.cpp -lm -fopenmp

For windows turn on, see slide 4 of the lecture
	-> in config props in VS


Coolest thing (possibly):


	omp_set_num_threads(NUMT);  // sets how many threads will be in the thread pool

	...

	#pragma omp parallel for
	for(int i = 0; i < arraySize; i++)
	{....}

	-> code starts as a single thread,
	then this uses NUMT threads to execute the loop simultaneously



Parallel programming background infomation

Lecture 1 (A?)
=======================================================

3 reasons:
	1) increase performance (more work same time)
	2) increase performance (take lest time to do the same amt of work)
	3) make some programming tasks more convenient to implement


3 types of parallelism:

	1) instruction level parallelism (ILP)
		-> out of order execution
			-> logic looks ahead to grab some code that it can execute while waiting on something that might be blocked (such as fetchign something that isn't in cache (getting from memory))

			-> if compiler does this = Static ILP
			-> if CPU chip = Dynamic ILP

		Note: we can't control this


	2) Data Level Parallelism (DLP)
		-> executing the same instructions on different parts of the data
			If we have a for loop on a large array and same instructions.. we can break this up!
				-> multiple threads on multiple cores

			We will use OpenMP with this!


	3) Thread Level Parallelism (TLP)
		-> executing DIFFERENT instructions
			Ex: variety of incoming transaction requests
		-> implies you have more threads than cores
		-> thread exec switches when a thread blocks or uses up its time slice

State:
	-> Registers
	-> Program Counter
	-> Stack Pointer

	(We will visit this again in multi-core programming)


Process:
	executes a program in memory

	keeps a state (see above)


Thread:
	separate indp procs, all execing a common pgm and sharing memory
		-> each thread has its own state

	-> only thing different is each thread has it's own stack

	can time share on a single processors
		(don't need multiple processors, but you can - MULTICORE COMING SOON!)



lecture 2
=============================================================================

Conflicts in multithreaded programs.. Thread safety
	-> can take a function that be used many times and won't alter the state of something

	What is not thread safe?

		Ex: char *strtok (char * str, const char * delims);

		-> this keeps internal state info.. if we alter that state in a 2nd thread, this screws with the first threads info.

		strtok_r returns the internal state (new version of strtok)
			-> so you can pass back when ready.. 'r' stands for "re-entrant"



Deadlock Fault Problems:
	Deadlock: 2 threads are each waiting for the other to do something


Race Condition Fault:
	-> where it matters which trhead gets to a particular piece of code first
	-> often comes about when on thread is modifying a var while the other thread is in the midst of using it

	Often fixed with Mutual Exclusion Locks (Mutexes)
		-> lock the stuff while we do something, then unlock and let the other grab it and do its thing

volatile keyword: used to let the compiler know that another thread might be changing a variable "in the background", so don't make any assumptions about what can be optimized away.
	Ex:  volatile int val = 0;
		w/o volatile a good compiler would know that val == 0



restrict keyword: w/o this could create race condition in the "optimized", but wrong code.
	-> this is a "promise" that the vars will never point to the same memory location


lecture 3
=======================================================================================

Functional (or Task) Decomposition:
	-> breaking a task into sub-tasks that represent separate fns
	-> primarily indpt


Domain (or Data) Decomposition
	-> breaking task into sub-tasks that represent separate sections of data
	Ex: big matrix solutions


Atomic: An operation that takes place to completion w/ no chance of being interrupted by another thread

Deterministic: same set of inputs always gives the same outputs

Reduction: Combining the results from multiple threads into a single sum or product, continuing to use multithreading. Typically performed so that it takes O(log_2 N) time instead of O(N) time

Fine-grained parallelism: breaking a task into lots of small tasks

Coarse-grained parallelism: breaking a task up into small number of large tasks

Barrier: A point in the pgm where ALL threads must reach before any of them are allowed to proceed


Fork-join: operationg whr multiple threads are created from a main thread.  All of those forked threads are expected to eventually finish and thus "join back up" w/ the main thread

Shared Variable: After a fork operation, a var which is shared among threads, i.e., has a single value

Private variable: After a fork op, a var which has a private copy w/in each thread

Static scheduling: dividng the total # of tasks T up so that each of N available threads has T/N sub-tasks to do

Dynamic scheduling: dividing the total # of tasks T up so that each of N available threads has less than T/N sub-tasks to do, and then doling out the remaining tasks to threads as they become available

Speed-up(N)   T_1 / T_N

Speed-up Efficiency Speed-up(N) / N






WEEK 2
---------------------------------------------------------------------------------
(lecture 4 was week 2 welcome)

lecture 5 - Open MP A
=================================================================================

both directive and library-based

shared executable, global memory, and heap

Not totally deterministic, so no guarantee identical behavior accross venders or hardware
	-> even multiple runs on same hardware

Doesn't guarantee order of execution either



Open MP usage:
g++ -o proj proj.cpp -lm -fopenmp

for intel optimization:
icpc -o proj proj.cpp -lm -fopenmp -align -qopt-report=3 -qopt-report-phase=vec


set num threads:
omp_set_num_threads(num);

ask how many cores this pgm has access to:
num = omp_get_num_procs();

how many threads using right now?:
num = omp_get_num_threads();

which thread one is?
me = omp_get_thread_num();



#pragma omp parallel default(none)
	-> creates the team of threads to exec
	-> threads won't be in order


lecture 6 - Open MP B
=========================================================================================


#pragma omp parallel for default(none)
	-> creates team of threads and divides them among the for loop passes w/ 1 line of code
	-> default(none) - could remove the implied barrier with this optional item

	default(none) aka no default
		-> ensures variables are explicitly declared inside and outside the threaded portion w/ shared or private

	private(x) - means each thread has its own copy of var x

	shared(x) - all threads shared a common x


	Ex:
	#pragma omp parallel for default(none), private(i,j), shared(x)


if you don't schedule with OpenMP, it defaults to static
	schedule(static [,chunksize])
	schedule(dynamic [,chunksize])
			-> chunksize defaults to 1


lecture 6 - Open MP C
=========================================================================================


Where can we see problems?

	Ex: adding a sum in the for loop that has be made to operate in parallel

		-> no guarantee the sum line is exec'd correctly

		-> no guarantee each thread will finish this line b4 some other thread interrupts it
			multiple Assembly code lines are usually is generated

		-> non-deterministic


	3 solutions:

		1)   #pragma omp atomic
			-> fixes non-deterministic problem
			-> tries to use a built in hardware instruction

		2)  #pragma omp critical
			-> disables scheduler interrupts during the critical section


				BEST METHOD
		3) #pragma omp parallel for reduction(+:sum),private(myPartialSum)
			....
			sum = sum + myPartialSum;

			-> in O(log_2 N) instead of O(N)
			-> reduction creates a sum private var for each thread
				-> adds each some in a binary fashion (hence O(log_2 N))



lecture 7 - Open MP D
===========================================================================================

Synchronization:

	Mutual Exclusion Locks (Mutexes)

		omp_init_lock(omp_lock_t *);
		omp_set_lock(omp_lock_t *);  // blocks if lock not available, sets and rtns when avail
		omp_unset_lock(omp_lock_t *);
		omp_test_lock(omp_lock_t *);  // if lock not avail, rtn 0, if avail, sets and retrns !0

Critical sections
	#pragma omp critical
		-> restricts exec to 1 thread at a time

	#pragma omp single
		-> restricts exe to a single thread ever

Barriers

	#pragma omp barrier
		-> forces each thread to wait here until all threads arrive
			-> implied w/ for loops and openMP sections, unless NOWAIT clause is used.


Creating sections of MP Code
	-> indp blocks of code, able to be assigned to separate threads if they are available

	#pragma omp parallel sections
	{
		#pragma omp section
		{
			Task 1
		}

		#pragma omp section
		{
			Task 2
		}
	}  // implied barrier here



OpenMP Tasks - works like a section, but created on the fly

	#pragma omp task
		code here

	Force a barrier
		#pragma omp taskwait


lecture 9 - Speedups and Amdahl's law
===========================================================================

Using n processors, speedup_n is:

	speedup_n = T1 / Tn... actually, I think this should be flipped with Tn on top. as speedup should be > 1
						-> if you take the inverse of this number, you get something > 1
							-> which is just the number of Tn / T1 anyway.

Speedup Efficiency_n:
		E_n = Speedup_n / n  (could be as high as 1)




Amdahl's Law:
	-> mainly used to predict the theoretical max speedup of a program processing using multiple processors

	Speedup_n = T1 / Tn
		= 1 /  [ (F_parallel / n) + F_sequential ]

		= 1 / [ (F_p / n) + (1 - F_p)]


			-> parallel potion can be cut down, but not sequential

	-> if we know the max F_p, we can calc the max speedup


	F_p =   n / (n-1)   * T1 - Tn / T1
		=  n / (n-1)  * ( 1 -  1 / speedup)



	maxspeedup = 1 / 1 - F_p



Gustafson's Obersvation:
	As amount of data increases, you can increase the parallel fraction
		-> data generally comes in on the parallel side rather than the sequential side
			-> it does still have to be read in, so there is some sequential, so this is somewhat theoretical

lecture 10 - Moore's Law and Multicore
===========================================================================

Moore's Law
	-> obersvation that "transistor density doubles every 1.5 years"
		-> does not necessarily mean clock speed does the same.. was a artifact of the above

After about 2003 - transistor density is still following the law, but clock speed and power cosnumption leveled off

Power Consumption (by the chip) is proportional to ClockSpeed ^ 2
	-> if you keep doubling clock speed, this would get incredibly hot
		For instance: by about mid 00's, it would be as hot as a nuclear reactor
			-> 2015 ish = sun's surface
































